{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af370b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# プロジェクトのルートディレクトリを取得\n",
    "project_root = Path(os.getcwd()).resolve().parent # 必要に応じて調整\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7b5e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1723383019166,
     "user": {
      "displayName": "関根伸吾",
      "userId": "16459890836042458182"
     },
     "user_tz": -540
    },
    "id": "b1d7b5e0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import zipfile\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import IPython.display\n",
    "from IPython.display import display, Markdown\n",
    "import subprocess\n",
    "import configparser\n",
    "from pdf2image import convert_from_path\n",
    "import faiss\n",
    "\n",
    "# from google.colab import userdata\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57301aa",
   "metadata": {
    "id": "a57301aa"
   },
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d5a57",
   "metadata": {},
   "source": [
    "### pathの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2defce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"..\"\n",
    "# 出力フォルダがなければ作成\n",
    "for dir_name in [\"process\", \"output\", \"models\"]:\n",
    "    if not os.path.exists(f\"{base_path}/{dir_name}\"):\n",
    "        os.makedirs(f\"{base_path}/{dir_name}\")\n",
    "\n",
    "# 入力フォルダと出力フォルダ\n",
    "input_folder = \"../documents/\"\n",
    "output_folder = \"../documents_md/\"\n",
    "\n",
    "# 出力フォルダがなければ作成\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# PDF ファイルを取得\n",
    "# 動作確認用に1ファイルに絞っている\n",
    "pdf_files = glob.glob(os.path.join(input_folder, \"*.pdf\"))\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGConfig:\n",
    "    \"\"\"\n",
    "    設定ファイル(config.ini)を読み込むクラス。\n",
    "    configファイルは下記のように構成されます。\n",
    "    ```\n",
    "    [DEFAULT]\n",
    "    URL=<url>\n",
    "    API_KEY=<api_key>\n",
    "    API_VERSION=<api_version>\n",
    "    CHAT_MODEL=<CHAT_MODEL>\n",
    "    EMBEDDING_MODEL=<EMBEDDING_MODEL>\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        self.config = self._load_config(config_path)\n",
    "\n",
    "    def _load_config(self, config_path):\n",
    "        \"\"\"\n",
    "        設定ファイルを読み込み、DEFAULTセクションを取得する。\n",
    "\n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス。\n",
    "\n",
    "        Returns:\n",
    "            ConfigParser.SectionProxy: DEFAULTセクションの設定。\n",
    "        \"\"\"\n",
    "        config_ini = configparser.ConfigParser()\n",
    "        config_ini.read(config_path, encoding=\"utf-8\")\n",
    "        return config_ini[\"DEFAULT\"]\n",
    "\n",
    "    @property\n",
    "    def azure_openai_endpoint(self):\n",
    "        \"\"\"Azure OpenAIエンドポイントURLを取得\"\"\"\n",
    "        return self.config[\"URL\"]\n",
    "\n",
    "    @property\n",
    "    def azure_openai_api_key(self):\n",
    "        \"\"\"Azure OpenAIのAPIキーを取得\"\"\"\n",
    "        return self.config[\"API_KEY\"]\n",
    "\n",
    "    @property\n",
    "    def api_version(self):\n",
    "        \"\"\"APIバージョンを取得\"\"\"\n",
    "        return self.config[\"API_VERSION\"]\n",
    "\n",
    "    @property\n",
    "    def CHAT_MODEL(self):\n",
    "        \"\"\"使用するチャットモデル名を取得\"\"\"\n",
    "        return self.config[\"CHAT_MODEL\"]\n",
    "\n",
    "    @property\n",
    "    def EMBEDDING_MODEL(self):\n",
    "        \"\"\"使用する埋め込みモデル名を取得\"\"\"\n",
    "        return self.config[\"EMBEDDING_MODEL\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定を読み込む\n",
    "config = RAGConfig('../config/config.ini')\n",
    "\n",
    "# Azure OpenAI クライアントを初期化（AzureOpenAI は別途定義されていることを前提）\n",
    "client = AzureOpenAI(\n",
    "    api_key=config.azure_openai_api_key,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_version=config.api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abe441",
   "metadata": {},
   "source": [
    "## vector_store作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd2be5",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b37a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 各 PDF に `yomitoku` を適用\n",
    "for pdf_file in pdf_files:\n",
    "    file = os.path.basename(pdf_file).split(\".\")[0]\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    command = f'yomitoku \"{pdf_file}\" -f md -o {output_folder}{file}'\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "print(\"すべての PDF を Markdown に変換しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30588b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 企業名付与\n",
    "concept_dicts = {\n",
    "    '../documents/9.pdf': 'ハウス食品グループ',\n",
    "     '../documents/8.pdf': 'サントリーグループ',\n",
    "     '../documents/16.pdf': '東急不動産ホールディングス',\n",
    "     '../documents/17.pdf': 'TOYOエンジニアリング株式会社',\n",
    "     '../documents/15.pdf': '全国保証株式会社',\n",
    "     '../documents/14.pdf': '髙松コンストラクショングループ',\n",
    "     '../documents/10.pdf': 'パナソニック',\n",
    "     '../documents/11.pdf': '株式会社メディアドゥ',\n",
    "     '../documents/13.pdf': 'ライフコーポレーション',\n",
    "     '../documents/12.pdf': 'モスグループ',\n",
    "     '../documents/19.pdf': '明治ホールディングス',\n",
    "     '../documents/18.pdf': '日清食品ホールディングス',\n",
    "     '../documents/6.pdf': 'クレハ',\n",
    "     '../documents/7.pdf': 'グローリー株式会社',\n",
    "     '../documents/5.pdf': 'キッツ株式会社',\n",
    "     '../documents/4.pdf': 'カゴメ株式会社',\n",
    "     '../documents/1.pdf': '株式会社4℃ホールディングス',\n",
    "     '../documents/3.pdf': '日産自動車',\n",
    "     '../documents/2.pdf': 'IHIグループ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    file = os.path.basename(pdf_file).split(\".\")[0]     \n",
    "    images = convert_from_path(pdf_file) \n",
    "    concept = concept_dicts[pdf_file]\n",
    "    for page, image in enumerate(images, start=1):\n",
    "        image_bytes = io.BytesIO()\n",
    "        image.save(image_bytes, format=\"JPEG\")\n",
    "        encode_image = base64.b64encode(image_bytes.getvalue()).decode('utf-8')    \n",
    "\n",
    "        md_path = f\"{output_folder}{file}/documents_{file}_p{page}.md\"\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            md_content = f.read()\n",
    "        text_data.append({\n",
    "            \"pdf_path\":pdf_file,\n",
    "            \"page\": page,\n",
    "            \"concept\": concept,\n",
    "            \"search\": f\"# {concept}\\n{md_content}\",\n",
    "            \"image\": encode_image\n",
    "        })\n",
    "\n",
    "rag_df = pd.DataFrame(text_data)\n",
    "rag_df.to_pickle(\"../process/rag_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830c972",
   "metadata": {
    "id": "0830c972"
   },
   "source": [
    "## ベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateVectorStore:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, client, config):\n",
    "        self.client = client  # Azure OpenAI クライアント\n",
    "        self.config = config  # チャットモデル名\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"\n",
    "        テキストを埋め込みベクトルに変換する。\n",
    "\n",
    "        Args:\n",
    "            text (str): 埋め込み対象のテキスト。\n",
    "\n",
    "        Returns:\n",
    "            np.array: 埋め込みベクトル。\n",
    "        \"\"\"\n",
    "        vector = (\n",
    "            self.client.embeddings.create(\n",
    "                input=text,\n",
    "                model=self.config.EMBEDDING_MODEL,\n",
    "            )\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "        return vector\n",
    "\n",
    "    def encode_questions(self, df):\n",
    "        \"\"\"\n",
    "        DataFrame内の質問テキストを埋め込みベクトルに変換する。\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): 'search'列に質問を含むDataFrame。\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 質問の埋め込みベクトルの配列。\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for question in tqdm(df[\"search\"]):\n",
    "            embedding = self.encode_text(question)\n",
    "            embeddings.append(embedding)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    def build_faiss_index(self, embeddings):\n",
    "        \"\"\"\n",
    "        埋め込みベクトルを使用してFAISSインデックスを構築する。\n",
    "\n",
    "        Args:\n",
    "            embeddings (np.ndarray): 埋め込みベクトルの配列。\n",
    "\n",
    "        Returns:\n",
    "            faiss.IndexFlatL2: 構築されたFAISSインデックス。\n",
    "        \"\"\"\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f456508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGProcessor を初期化\n",
    "create_vector_store = CreateVectorStore(\n",
    "    client=client,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 質問の埋め込みベクトルを作成\n",
    "question_embeddings = create_vector_store.encode_questions(rag_df)\n",
    "\n",
    "# FAISSインデックスを構築\n",
    "index = create_vector_store.build_faiss_index(question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1be14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df[\"context\"] = rag_df[\"search\"]\n",
    "# FAISSモデルをファイルに保存\n",
    "faiss_model = {\n",
    "    \"index\": index,\n",
    "    \"rag_df\": rag_df,\n",
    "}\n",
    "\n",
    "with open(f\"{base_path}/models/faiss_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(faiss_model, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774c319",
   "metadata": {},
   "source": [
    "## 文章生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c740aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_path}/models/faiss_model.pkl\", \"rb\") as f:\n",
    "    faiss_model = pickle.load(f)\n",
    "index = faiss_model[\"index\"]\n",
    "rag_df = faiss_model[\"rag_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912c0f-af82-49bd-b2e0-4c7455d945d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認用に検証用に5件に絞っている\n",
    "query_df = pd.read_csv(f\"{base_path}/input/query.csv\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710900f0-bd99-43d6-9c03-4cec4aae0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Generater():\n",
    "    def __init__(self, client, config, index):\n",
    "        self.client = client  # Azure OpenAI クライアント\n",
    "        self.config = config  # チャットモデル名\n",
    "        self.index = index\n",
    "        \n",
    "    def extract_rag_indices(self, question, k=2):\n",
    "        query_embedding = (\n",
    "            self.client.embeddings.create(\n",
    "                input=question,\n",
    "                model=self.config.EMBEDDING_MODEL,\n",
    "            )\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding], dtype=np.float32), k=k\n",
    "        )\n",
    "        return indices[0]\n",
    "    \n",
    "    def generate_process_answer(\n",
    "            self,\n",
    "            question,\n",
    "            text=None,\n",
    "            max_tokens=1000):\n",
    "\n",
    "        text_prefix = f\"\"\"\n",
    "        次のテキストは関連したドキュメントをMarkdownに変換したものである。\n",
    "        これを元に、質問に対して**計算過程を明確に説明し、最後に答えを出せ**。\n",
    "\n",
    "        - **数値を求める場合**\n",
    "          1. 必要な数値をすべてリストアップする。\n",
    "          2. 計算式を明示し、途中計算をすべて書く。必ず下記に記載の計算規則に従うこと。\n",
    "          3. **計算過程をステップごとに示し、答えを最後に書く**。\n",
    "          4. **もう一度質問と計算規則をかきだし、照らし合わせて問われていることが間違っていないか確認し、誤りを防ぐ**。\n",
    "          もし質問に正確に答えることができない、または推測が入っている場合は**「分かりません」**と答えること。\n",
    "        - **計算規則**\n",
    "            - 小数第n位を四捨五入する場合は第n位で四捨五入を行い、答えは小数第(n-1)桁とすること。\n",
    "            - 最大値を求める場合は、リストアップした中から上位を算出し、その中で最大のものを求めること。\n",
    "        - **情報が不足している場合**\n",
    "          - **「分かりません」** と明確に回答し、足りない情報を指摘する\"\"\",\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"\n",
    "                {text_prefix}\n",
    "                {question}\n",
    "                参考：\n",
    "                {text}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config.CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":content\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0,        \n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        if answer is None or answer.strip() == \"\":\n",
    "            answer = \"分かりません\"\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def generate_answer(self, question, process_answer, max_tokens=54):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config.CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"\"\"\n",
    "                            次のテキストは質問に理由をつけて答えてもらったものである。\n",
    "                            これをもとに質問に答えを単語のみ答えよ。複数ある場合は複数の単語で答えよ。\n",
    "                            情報が不足している場合には「分かりません」と答えよ。\n",
    "                            質問：{question}\n",
    "                            理由と答え：{process_answer}\n",
    "                            \"\"\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0,        \n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        if answer is None or answer.strip() == \"\":\n",
    "            answer = \"分かりません\"\n",
    "        answer = answer.strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f23af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add04a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rag_generator = RAG_Generater(client, config, index)\n",
    "pred_answers = []\n",
    "for n, row in query_df.iterrows():\n",
    "    question = row.problem\n",
    "    rag_indices = rag_generator.extract_rag_indices(question)\n",
    "    for i, row2 in rag_df.iloc[rag_indices].iterrows():\n",
    "        encode_image, search = row2.image, row2.search\n",
    "        process_answer = rag_generator.generate_process_answer(question, text=search)\n",
    "        print(n, question, row2.pdf_path, row2.page)\n",
    "        display(Markdown(process_answer))\n",
    "        if \"分かりません\" not in process_answer:\n",
    "            final_answer = rag_generator.generate_answer(question, process_answer, max_tokens=54)\n",
    "            pred_answers.append([row2.pdf_path, row2.page, encode_image, search, process_answer, final_answer])       \n",
    "            break\n",
    "    else:\n",
    "        pred_answers.append([row2.pdf_path, row2.page, encode_image, search, process_answer, \"分かりません\"]) \n",
    "query_df[[\"pdf_path\", \"page\", \"encode_image\", \"text\", \"pred_process_answer\", \"pred_final_answer\"]] = pred_answers\n",
    "query_df[\"pred_final_answer\"] = query_df[\"pred_final_answer\"].str.replace(\"\\n\",\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,row in query_df.iterrows():\n",
    "    print(_,row.problem)\n",
    "    display(Markdown(row.pred_final_answer)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e9829",
   "metadata": {},
   "source": [
    "## 出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee922d3-519d-42fd-b98f-e1eb6fa3894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = f\"{base_path}/output/predictions.csv\"\n",
    "output_zip_path = f\"{base_path}/output/predictions.zip\"\n",
    "query_df[[\"index\", \"pred_final_answer\"]].to_csv(output_csv_path, index=False, header=False)\n",
    "\n",
    "with zipfile.ZipFile(\n",
    "        output_zip_path, 'w',\n",
    "        compression=zipfile.ZIP_DEFLATED,\n",
    "        compresslevel=9) as zf:\n",
    "    zf.write(output_csv_path, arcname=os.path.basename(output_csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7bd43-6a1b-4c08-8254-170f1593b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
